{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "from shutil import copy2, move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ROOT_DIR = os.getcwd()\n",
    "DATASET_DIR = os.path.join(ROOT_DIR, \"mapillary_dataset\")\n",
    "ORIGINAL_20000_IMG = os.path.join(DATASET_DIR, \"original_20000\", \"images\")\n",
    "ORIGINAL_20000_INS = os.path.join(DATASET_DIR, \"original_20000\", \"instances\")\n",
    "\n",
    "DS_STORE = '.DS_Store'\n",
    "\n",
    "intersection_class_ids = [0, 19, 20, 21, 33, 38, 48, 54, 55, 57, 61]\n",
    "instance_class_ids = [0, 1, 8, 19, 20, 21, 22, 23, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 59, 60, 61, 62]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write a list of files to .txt\n",
    "def Write_TXT(file_names, dst, txt_name):\n",
    "    if not os.path.exists(dst):\n",
    "            os.makedirs(dst)\n",
    "    with open(os.path.join(dst, txt_name), 'w') as the_file:\n",
    "        for file_name in file_names:\n",
    "            the_file.write(file_name.rsplit(\".\", 1)[0])\n",
    "            the_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Read_TXT(txt_path):\n",
    "    with open(txt_path) as f:\n",
    "        content = f.readlines()\n",
    "    # you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "    content = [x.strip() for x in content]\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python code t get difference of two lists\n",
    "# Using set()\n",
    "def Diff(li1, li2):\n",
    "    return (list(set(li1) - set(li2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copy a list of files to a desired directory\n",
    "def Copy_Files(dataset_name, subset_name, file_names):\n",
    "    for file_name in file_names:\n",
    "        src = os.path.join(ORIGINAL_20000_IMG, file_name + \".jpg\")\n",
    "        dst = os.path.join(DATASET_DIR, dataset_name, subset_name, \"images\")\n",
    "        if not os.path.exists(dst):\n",
    "            os.makedirs(dst)\n",
    "        copy2(src, dst)\n",
    "\n",
    "        ins_name = file_name.rsplit(\".\", 1)[0] + \".png\"\n",
    "        src = os.path.join(ORIGINAL_20000_INS, ins_name)\n",
    "        dst = os.path.join(DATASET_DIR, dataset_name, subset_name, \"instances\")\n",
    "        if not os.path.exists(dst):\n",
    "            os.makedirs(dst)\n",
    "        copy2(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Read_Dataset(txt_path):\n",
    "    if os.path.exists(txt_path):\n",
    "        all_files = Read_TXT(txt_path)\n",
    "    else:\n",
    "        IMG_DIR = os.path.join(txt_path.rsplit(\".\", 1)[0], \"images\")\n",
    "        print(IMG_DIR)\n",
    "        all_files = next(os.walk(IMG_DIR))[2]\n",
    "        if DS_STORE in all_files:\n",
    "            all_files.remove(DS_STORE)\n",
    "        Write_TXT(all_files, IMG_DIR, txt_path)\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select desired number of examples whose ground truth masks are non-trivial (defined by threshold)\n",
    "# this function was modified from dataset_clean.ipynb\n",
    "def Select_Examples(available_files, num_files_needed, threshold, class_ids):\n",
    "    accepted = []\n",
    "    rejected = []\n",
    "    idx = 0\n",
    "    while len(accepted) < num_files_needed:\n",
    "        if idx >= len(available_files):\n",
    "            print(\"not enough avaialble files!\")\n",
    "            break\n",
    "            \n",
    "        file_name = available_files[idx]\n",
    "        if file_name != '.DS_Store':\n",
    "            IMG_PATH = os.path.join(ORIGINAL_20000_IMG, file_name)\n",
    "            ins_name = file_name.rsplit(\".\", 1)[0] + \".png\"\n",
    "            INS_PATH = os.path.join(ORIGINAL_20000_INS, ins_name)\n",
    "            instance_image = Image.open(INS_PATH)\n",
    "            \n",
    "            # convert labeled data to numpy arrays for better handling\n",
    "            instance_array = np.array(instance_image, dtype=np.uint16)\n",
    "\n",
    "            instances = np.unique(instance_array)\n",
    "            instaces_count = instances.shape[0]\n",
    "\n",
    "            label_ids = instances // 256\n",
    "            label_id_count = np.unique(label_ids).shape[0]\n",
    "\n",
    "            mask_count = 0\n",
    "            for instance in instances:\n",
    "                label_id = instance // 256\n",
    "                if label_id in class_ids:\n",
    "                    m = np.zeros((instance_array.shape[0], instance_array.shape[1]), dtype=np.uint8)\n",
    "                    m[instance_array == instance] = 1\n",
    "                    m_size = np.count_nonzero(m == 1)\n",
    "\n",
    "                    # only load mask greater than threshold size, \n",
    "                    # otherwise bounding box with area zero causes program to crash\n",
    "                    if m_size > threshold:\n",
    "                        mask_count = mask_count + 1\n",
    "            if mask_count == 0:\n",
    "                rejected.append(file_name)\n",
    "            else:\n",
    "                accepted.append(file_name)\n",
    "        idx = idx + 1\n",
    "        print('Accepted {}/{}, rejected {}\\r'.format(len(accepted), num_files_needed, len(rejected)), end='', )\n",
    "        \n",
    "\n",
    "        with open(os.path.join(DATASET_DIR, \"progress.txt\"), 'w') as the_file:\n",
    "            the_file.write('Accepted {}/{}, rejected {}\\r'.format(len(accepted), num_files_needed, len(rejected)))\n",
    "            the_file.write('\\n')\n",
    "            \n",
    "    return accepted, rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Build_Dataset(available_files, dataset_name, subset_name, threshold, size, rebuild = False, class_ids = intersection_class_ids):\n",
    "    accepted = []\n",
    "    rejected = []\n",
    "\n",
    "    subset_dir = os.path.join(DATASET_DIR, dataset_name, subset_name)\n",
    "    accepted_txt = os.path.join(subset_dir, \"accepted.txt\")\n",
    "    rejected_txt = os.path.join(subset_dir, \"rejected.txt\")\n",
    "\n",
    "    if rebuild:\n",
    "        accepted, rejected = Select_Examples(available_files, size, threshold, class_ids)\n",
    "\n",
    "        Write_TXT(accepted, subset_dir, \"accepted.txt\")\n",
    "        Write_TXT(rejected, subset_dir, \"rejected.txt\")\n",
    "\n",
    "        Copy_Files(subset_dir, \"accepted\", accepted)\n",
    "        Copy_Files(subset_dir, \"rejected\", rejected)\n",
    "    else:\n",
    "        accepted = Read_Dataset(accepted_txt)\n",
    "        rejected = Read_Dataset(rejected_txt)\n",
    "    \n",
    "    used = accepted + rejected\n",
    "    available_files = Diff(available_files, used)\n",
    "\n",
    "    print('{} accepted {} and rejected {}'.format(dataset_name, len(accepted), len(rejected)))\n",
    "    print('{} images still available'.format(len(available_files)))\n",
    "    \n",
    "    return available_files, accepted, rejected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the original 20k images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 images were found in the mapillary dataset\n"
     ]
    }
   ],
   "source": [
    "txt_20k = os.path.join(DATASET_DIR, \"original_20000.txt\")\n",
    "all_files = Read_Dataset(txt_20k)\n",
    "print('{} images were found in the mapillary dataset'.format(len(all_files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS run 1 (mask threshold 64^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AWS_RUN_1 = os.path.join(DATASET_DIR, \"AWS_run_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4608 images were used in AWS run 1 (train + dev)\n"
     ]
    }
   ],
   "source": [
    "txt_path = os.path.join(DATASET_DIR, \"AWS_run_1\", \"train_4096.txt\")\n",
    "train_file_names = Read_Dataset(txt_path)\n",
    "\n",
    "txt_path = os.path.join(DATASET_DIR, \"AWS_run_1\", \"dev_512.txt\")\n",
    "dev_file_names = Read_Dataset(txt_path)\n",
    "\n",
    "used_files = list(set(train_file_names + dev_file_names))\n",
    "    \n",
    "print('{} images were used in AWS run 1 (train + dev)'.format(len(used_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15392 images still available\n"
     ]
    }
   ],
   "source": [
    "# take difference between two lists\n",
    "available_files = Diff(all_files, used_files)\n",
    "print('{} images still available'.format(len(available_files)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make it reproducible\n",
    "#### ref: https://cs230-stanford.github.io/train-dev-test-split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make sure that the filenames have a fixed order before shuffling\n",
    "available_files.sort()  \n",
    "\n",
    "# fix the random seed\n",
    "random.seed(0)\n",
    "\n",
    "# shuffles the ordering of filenames (deterministic given the chosen seed)\n",
    "random.shuffle(available_files) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Run 2 (mask threshold 32^2)¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS_run_2 accepted 4096 and rejected 226\n",
      "11070 images still available\n"
     ]
    }
   ],
   "source": [
    "available_files, AWS_run_2_train, AWS_run_2_train_rejected = Build_Dataset(available_files = available_files,\n",
    "                                                                           dataset_name = \"AWS_run_2\", \n",
    "                                                                           subset_name = \"train_4096\",\n",
    "                                                                           threshold = 32 * 32, \n",
    "                                                                           size = 4096, \n",
    "                                                                           rebuild = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS_run_2 accepted 512 and rejected 39\n",
      "10519 images still available\n"
     ]
    }
   ],
   "source": [
    "available_files, AWS_run_2_dev, AWS_run_2_dev_rejected = Build_Dataset(available_files = available_files,\n",
    "                                                                           dataset_name = \"AWS_run_2\", \n",
    "                                                                           subset_name = \"dev_512\",\n",
    "                                                                           threshold = 32 * 32, \n",
    "                                                                           size = 512, \n",
    "                                                                           rebuild = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Run 3 (Full dataset, mask threshold 32^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 images were found in the mapillary dataset\n"
     ]
    }
   ],
   "source": [
    "txt_20k = os.path.join(DATASET_DIR, \"original_20000.txt\")\n",
    "all_files = Read_Dataset(txt_20k)\n",
    "print('{} images were found in the mapillary dataset'.format(len(all_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make sure that the filenames have a fixed order before shuffling\n",
    "all_files.sort()  \n",
    "\n",
    "# fix the random seed\n",
    "random.seed(230)\n",
    "\n",
    "# shuffles the ordering of filenames (deterministic given the chosen seed)\n",
    "random.shuffle(all_files) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS_run_3 accepted 1024 and rejected 44\n",
      "18932 images still available\n"
     ]
    }
   ],
   "source": [
    "available_files, AWS_run_3_dev, AWS_run_3_dev_rejected = Build_Dataset(available_files = all_files,\n",
    "                                                                           dataset_name = \"AWS_run_3\", \n",
    "                                                                           subset_name = \"dev_1024\",\n",
    "                                                                           threshold = 32 * 32, \n",
    "                                                                           size = 1024, \n",
    "                                                                           rebuild = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS_run_3 accepted 1024 and rejected 36\n",
      "17872 images still available\n"
     ]
    }
   ],
   "source": [
    "available_files, AWS_run_3_test, AWS_run_3_test_rejected = Build_Dataset(available_files = available_files,\n",
    "                                                                           dataset_name = \"AWS_run_3\", \n",
    "                                                                           subset_name = \"test_1024\",\n",
    "                                                                           threshold = 32 * 32, \n",
    "                                                                           size = 1024, \n",
    "                                                                           rebuild = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS_run_3 accepted 16384 and rejected 741\n",
      "747 images still available\n"
     ]
    }
   ],
   "source": [
    "available_files, AWS_run_3_train, AWS_run_3_train_rejected = Build_Dataset(available_files = available_files,\n",
    "                                                                           dataset_name = \"AWS_run_3\", \n",
    "                                                                           subset_name = \"train_16384\",\n",
    "                                                                           threshold = 32 * 32, \n",
    "                                                                           size = 16384, \n",
    "                                                                           rebuild = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Run 4 (Full dataset, 38 classes, mask threshold 32^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 images were found in the mapillary dataset\n"
     ]
    }
   ],
   "source": [
    "txt_20k = os.path.join(DATASET_DIR, \"original_20000.txt\")\n",
    "all_files = Read_Dataset(txt_20k)\n",
    "print('{} images were found in the mapillary dataset'.format(len(all_files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make sure that the filenames have a fixed order before shuffling\n",
    "all_files.sort()  \n",
    "\n",
    "# fix the random seed\n",
    "random.seed(230)\n",
    "\n",
    "# shuffles the ordering of filenames (deterministic given the chosen seed)\n",
    "random.shuffle(all_files) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS_run_4 accepted 1024 and rejected 2\n",
      "18974 images still available\n"
     ]
    }
   ],
   "source": [
    "available_files, AWS_run_4_dev, AWS_run_4_dev_rejected = Build_Dataset(available_files = all_files,\n",
    "                                                                           dataset_name = \"AWS_run_4\", \n",
    "                                                                           subset_name = \"dev_1024\",\n",
    "                                                                           threshold = 32 * 32, \n",
    "                                                                           size = 1024, \n",
    "                                                                           rebuild = False,\n",
    "                                                                           class_ids = instance_class_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS_run_4 accepted 1024 and rejected 5\n",
      "17945 images still available\n"
     ]
    }
   ],
   "source": [
    "available_files, AWS_run_4_test, AWS_run_4_test_rejected = Build_Dataset(available_files = available_files,\n",
    "                                                                           dataset_name = \"AWS_run_4\", \n",
    "                                                                           subset_name = \"test_1024\",\n",
    "                                                                           threshold = 32 * 32, \n",
    "                                                                           size = 1024, \n",
    "                                                                           rebuild = False,\n",
    "                                                                           class_ids = instance_class_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS_run_4 accepted 16384 and rejected 52\n",
      "1509 images still available\n"
     ]
    }
   ],
   "source": [
    "available_files, AWS_run_4_train, AWS_run_4_train_rejected = Build_Dataset(available_files = available_files,\n",
    "                                                                           dataset_name = \"AWS_run_4\", \n",
    "                                                                           subset_name = \"train_16384\",\n",
    "                                                                           threshold = 32 * 32, \n",
    "                                                                           size = 16384, \n",
    "                                                                           rebuild = False,\n",
    "                                                                           class_ids = instance_class_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train set into 8 parts of 2048 images to upload individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_name = \"AWS_run_4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# copy files from original_20000 if messed up\n",
    "# Copy_Files(os.path.join(DATASET_DIR, dataset_name, \"train_16384\"), \"accepted\", AWS_run_4_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide images into 8 folders\n",
    "files_to_move = AWS_run_4_train\n",
    "part_size = 2048\n",
    "directory = os.path.join(DATASET_DIR, dataset_name, \"train_16384\", \"accepted\", \"images\")\n",
    "num_parts = int(len(files_to_move) / part_size)\n",
    "\n",
    "num_files = next(os.walk(directory))[1]\n",
    "if (len(num_files) == len(files_to_move)):\n",
    "    for i in range(num_parts):\n",
    "        part = files_to_move[:part_size]\n",
    "        part_dir = os.path.join(directory, \"part_\" + str(i))\n",
    "        if not os.path.exists(part_dir):\n",
    "            os.makedirs(part_dir)\n",
    "        for file in part:\n",
    "            src = os.path.join(directory, file + '.jpg')\n",
    "            move(src, part_dir)\n",
    "        files_to_move = Diff(files_to_move, part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_files in part_0: 2048\n",
      "num_files in part_1: 2048\n",
      "num_files in part_2: 2049\n",
      "num_files in part_3: 2048\n",
      "num_files in part_4: 2048\n",
      "num_files in part_5: 2048\n",
      "num_files in part_6: 2048\n",
      "num_files in part_7: 2048\n"
     ]
    }
   ],
   "source": [
    "# check the number of images in each folder\n",
    "directory = os.path.join(DATASET_DIR, dataset_name, \"train_16384\", \"accepted\", \"images\")\n",
    "folders = next(os.walk(directory))[1]\n",
    "folders.sort()  \n",
    "for folder in folders:\n",
    "    part_dir = os.path.join(directory, folder)\n",
    "    files = next(os.walk(part_dir))[2]\n",
    "    if DS_STORE in files:\n",
    "        files.remove(DS_STORE)\n",
    "    print(\"num_files in {}: {}\".format(folder, len(files)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # generate datasets sequentially withut rejecting anything (not recommended)\n",
    "# TRAIN_BATCH_SIZE = 4096\n",
    "# DEV_BATCH_SIZE = 512\n",
    "# TEST_SET_SIZE = len(available_files) - (TRAIN_BATCH_SIZE + DEV_BATCH_SIZE) * 3\n",
    "\n",
    "# start = 0\n",
    "# AWS_run_2_train = available_files[:TRAIN_BATCH_SIZE]\n",
    "# AWS_run_2_dev   = available_files[TRAIN_BATCH_SIZE : TRAIN_BATCH_SIZE + DEV_BATCH_SIZE]\n",
    "\n",
    "# AWS_run_3_train = available_files[TRAIN_BATCH_SIZE + DEV_BATCH_SIZE : TRAIN_BATCH_SIZE * 2 + DEV_BATCH_SIZE]\n",
    "# AWS_run_3_dev   = available_files[TRAIN_BATCH_SIZE * 2 + DEV_BATCH_SIZE : (TRAIN_BATCH_SIZE + DEV_BATCH_SIZE) * 2]\n",
    "\n",
    "\n",
    "# AWS_run_4_train = available_files[(TRAIN_BATCH_SIZE + DEV_BATCH_SIZE) * 2 : TRAIN_BATCH_SIZE * 3 + DEV_BATCH_SIZE * 2]\n",
    "# AWS_run_4_dev   = available_files[TRAIN_BATCH_SIZE * 3 + DEV_BATCH_SIZE *2 : (TRAIN_BATCH_SIZE + DEV_BATCH_SIZE) * 3]\n",
    "\n",
    "# assert(len(AWS_run_2_train) == 4096)\n",
    "# assert(len(AWS_run_3_train) == 4096)\n",
    "# assert(len(AWS_run_4_train) == 4096)\n",
    "\n",
    "# assert(len(AWS_run_2_dev) == 512)\n",
    "# assert(len(AWS_run_3_dev) == 512)\n",
    "# assert(len(AWS_run_4_dev) == 512)\n",
    "\n",
    "# test_set = available_files[(TRAIN_BATCH_SIZE + DEV_BATCH_SIZE) * 3 :]\n",
    "# print(\"{} images in the test set\".format(len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
